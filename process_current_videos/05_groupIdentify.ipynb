{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43086a90-93ea-4625-95d0-9b0b8b69a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from mpl_toolkits import mplot3d\n",
    "from shapely.geometry import Point, Polygon, MultiPoint, LineString\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "from itertools import combinations\n",
    "\n",
    "from datetime import datetime, timezone, tzinfo\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from fiona.crs import from_epsg\n",
    "from sklearn.cluster import DBSCAN\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import networkx as nx\n",
    "\n",
    "font = FontProperties()\n",
    "font.set_family('serif')\n",
    "font.set_name('Times New Roman')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "sns.color_palette(\"hls\", 4)\n",
    "sns.set_context(\"paper\", rc={\"font.size\":10,\"axes.titlesize\":12,\"axes.labelsize\":12})\n",
    "from shapely.geometry import Point\n",
    "globalcrs = \"EPSG:3857\"\n",
    "def imshow(image, show_axes = False, quiet = False):\n",
    "    if len(image.shape) == 3:\n",
    "      # Height, width, channels\n",
    "      # Assume BGR, do a conversion since \n",
    "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "      # Height, width - must be grayscale\n",
    "      # convert to RGB, since matplotlib will plot in a weird colormap (instead of black = 0, white = 1)\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    # Draw the image\n",
    "    plt.imshow(image)\n",
    "    if not show_axes:\n",
    "        # We'll also disable drawing the axes and tick marks in the plot, since it's actually an image\n",
    "        plt.axis('off')\n",
    "    if not quiet:\n",
    "        # Make sure it outputs\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10332e9f-b837-4afc-b9b4-b0c6fbd0ec3e",
   "metadata": {},
   "source": [
    "# Goal\n",
    "1. Simplify the process to identify people in groups\n",
    "2. export sample file to verify the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REVISED 2023-06-24\n",
    "resultfolder = \"../../_data/10_clean/03_individual/current/20230711\"\n",
    "if not os.path.exists(resultfolder):\n",
    "  os.mkdir(resultfolder)\n",
    "  \n",
    "root = \"../../_data/05_tracking_result_projected/step1_speed_vector/current\"\n",
    "files = glob.glob(root + \"/*.csv\")\n",
    "projectionls = {\n",
    "    \"bryant_park\":2263,\n",
    "   'Met':3857,\n",
    " 'Chestnut':3857,\n",
    " 'Downtown':3857\n",
    "}\n",
    "pathdf = pd.DataFrame(files, columns=[\"path\"])\n",
    "pathdf['video_group_name'] = pathdf['path'].apply(lambda x: \"_\".join(x.split(\"/\")[-1].split(\".\")[0].split(\"_\")[:-1]))\n",
    "pathdf['loc'] = pathdf['video_group_name'].apply(lambda x: x[:-15])\n",
    "pathdf['coord'] = pathdf['loc'].apply(lambda x: projectionls[x])\n",
    "pathdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1211d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all video paths\n",
    "videofolder = \"../../_data/00_raw/videos_current_highres\"\n",
    "videos = glob.glob(os.path.join(videofolder, \"*/*.avi\"))\n",
    "videos2 = glob.glob(os.path.join(videofolder, \"*/*/*.avi\"))\n",
    "videos = videos+videos2\n",
    "# file_path = \"../data/00_raw/videos_current_highres/\"+videoname+\".avi\"\n",
    "# video, fps, size = getbasics(file_path)\n",
    "video_df = pd.DataFrame({\n",
    "    \"video_name\": [os.path.basename(x) for x in videos],\n",
    "    \"video_path\": videos,\n",
    "    \"video_id\": [os.path.basename(x).split(\".\")[0] for x in videos],\n",
    "})\n",
    "video_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "065674dd-bfa6-4ba1-8956-9a67c41d07bb",
   "metadata": {},
   "source": [
    "## Method 1\n",
    "1. Two/more people stay in the same DB cluster for more than 3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f9ecc-5b32-480c-9016-9c68667fb9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatecluster(df, dis, epsg):\n",
    "    \"\"\"\n",
    "    This function go through each frame and run DBscan based on different Distance\n",
    "    threshod\n",
    "\n",
    "    \"\"\"\n",
    "    predlist = []\n",
    "    for f in tqdm(df['frame_id'].unique()):\n",
    "        preDF = df[df['frame_id']==f].reset_index(drop = True)\n",
    "        X = preDF[[f\"x_{epsg}\", f\"y_{epsg}\"]].values\n",
    "\n",
    "        # eps : maximum distance between two samples\n",
    "        # Here we use pixel distance for ease of visualization\n",
    "        # 12 pixel for 0.5m, given hunman to human interaction distance maximum as 1.2 meter\n",
    "        # https://en.wikipedia.org/wiki/Proxemics#:~:text=Hall%20described%20the%20interpersonal%20distances,and%20(4)%20public%20space.\n",
    "\n",
    "        clustering = DBSCAN(eps = dis, min_samples = 2).fit(X)\n",
    "        pred = clustering.labels_\n",
    "        predlist.append(pred)\n",
    "    \n",
    "    allpred = np.concatenate(predlist, axis=0)\n",
    "    return allpred\n",
    "\n",
    "\n",
    "def generate_social(traceGDF, epsg, dis = 1.9):\n",
    "    # dislist = [1.5, 2]\n",
    "    # distlistdict = {\n",
    "    #     2263: [1.2, 3.6], # feet\n",
    "    #     3857: [0.6, 1.5] # meter\n",
    "    # }\n",
    "    \"\"\"\n",
    "    d is the distance threshold for DBscan at meter\n",
    "    \"\"\"\n",
    "    traceGDF = traceGDF.sort_values(['frame_id', 'track_id']).reset_index(drop = True)\n",
    "    clusterlabel = generatecluster(traceGDF, dis, epsg)\n",
    "    FPre = pd.DataFrame(clusterlabel, columns = ['Social'])\n",
    "    DBcluster = pd.concat([traceGDF, FPre], axis = 1)\n",
    "\n",
    "\n",
    "    DBcluster['frame_id'] = DBcluster['frame_id'].astype(int)\n",
    "    # Drop those clusterlabel == -1, create a spatial cluster id\n",
    "    DBcluster['group_id_social'] = DBcluster['frame_id'].astype(str) + '_' +DBcluster['Social'].astype(str)\n",
    "    DBcluster['group_id_social'] = np.where(DBcluster['Social']==-1, np.nan, DBcluster['group_id_social'])\n",
    "    DBSocial = DBcluster[DBcluster['Social']!=-1].reset_index(drop = True)\n",
    "    # os.makedirs(outfolder+\"/step1_dbscan\")\n",
    "    DBSocial['group_id_social'] = DBSocial['group_id_social'].astype(str)\n",
    "    return DBSocial, DBcluster\n",
    "\n",
    "\n",
    "def valid_link(DBSocial, x, y, thred = 0.1, n = 1):\n",
    "    samplegroup = DBSocial[DBSocial['track_id'].isin([x,y])]\n",
    "    # calculate the speed_x, speed_y correlation between track 10 and 11\n",
    "    df_wide = samplegroup.pivot(index = 'frame_id', \n",
    "                                columns = 'track_id', \n",
    "                                values = [f'speed_x_{n}s', f'speed_y_{n}s',f'speed_{n}s']).reset_index()\n",
    "    # calculate the correlation\n",
    "    df_wide = df_wide.dropna()\n",
    "    coor1 = df_wide[(f\"speed_x_{n}s\",x)].corr(df_wide[(f\"speed_x_{n}s\",y)])\n",
    "    coor2 = df_wide[(f\"speed_y_{n}s\",x)].corr(df_wide[(f\"speed_y_{n}s\",y)])\n",
    "    coor3 = df_wide[(f\"speed_{n}s\",x)].corr(df_wide[(f\"speed_{n}s\",y)])\n",
    "    if coor1>thred and coor2>thred:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def getuvperframe(testdf, iditem):\n",
    "#     testdf = effDF[effDF['frame_id'] == frameid]\n",
    "    U = []\n",
    "    V = []\n",
    "    groupid = []\n",
    "    for i, group in tqdm(testdf.groupby([iditem])['track_id']):\n",
    "\n",
    "    # generate all combinations without replacement \n",
    "    # from the group of similar column pairs\n",
    "        for u, v in itertools.combinations(group, 2):\n",
    "            U.append(u)\n",
    "            V.append(v)\n",
    "            groupid.append(i)\n",
    "            \n",
    "    dfframe = pd.DataFrame({'u':U,\n",
    "             'v':V,\n",
    "              iditem: groupid\n",
    "             })\n",
    "\n",
    "    return dfframe\n",
    "\n",
    "def get_links(traceGDF, epsg=3857, timethred = 4):\n",
    "\n",
    "    # traceGDF = get_speed_vector(traceGDF)\n",
    "    # create spatial cluster\n",
    "    DBSocial, DBcluster = generate_social(traceGDF, epsg)\n",
    "\n",
    "\n",
    "    # two person appear together for at least 5 second (60*2 frame per second )\n",
    "    # or half of the appearing time\n",
    "    \n",
    "    selp = 'social'\n",
    "    # selp = 'personal_far'\n",
    "    iditem = 'group_id_{}'.format(selp)\n",
    "    df_links = getuvperframe(DBSocial, iditem)\n",
    "\n",
    "    # First Aggregation, disregard time continuity, only consider frequency\n",
    "    df_links = df_links.groupby(['u', 'v']).size().reset_index().rename(columns={0: 'weight'})\n",
    "\n",
    "    fps = 29.9\n",
    "    # qualified link --> staying together more than 2.5 seconds and speed correlation higher than 10%\n",
    "    # and the speed vector (x,y) have correlation more than 10%\n",
    "\n",
    "    df_links['valid'] = df_links.apply(lambda x: valid_link(DBSocial, x['u'], x['v']), axis = 1)\n",
    "    df_links_valid = df_links[(df_links['valid']==True)&(df_links['weight'] > timethred*fps)].reset_index(drop = True)\n",
    "    \n",
    "    return DBSocial, DBcluster, df_links_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a8f82-c4d5-4385-bf78-b33d5183f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import edge_betweenness_centrality as betweenness\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "# set up loops\n",
    "from datetime import datetime\n",
    "import json\n",
    "# use girvan_newman for community detection first\n",
    "# df_plot = newlinks.reset_index(drop=True)\n",
    "\n",
    "def most_central_edge(G):\n",
    "    centrality = betweenness(G, weight=\"weight\")\n",
    "    return max(centrality, key=centrality.get)\n",
    "\n",
    "\n",
    "def getcommunity(df_links):\n",
    "    G_plot = nx.Graph()\n",
    "    for link in tqdm(df_links.index):\n",
    "        G_plot.add_edge(df_links.iloc[link]['u'],\n",
    "                    df_links.iloc[link]['v'],\n",
    "                    weight = df_links.iloc[link]['weight'])\n",
    "\n",
    "    communities = girvan_newman(G_plot, most_valuable_edge = most_central_edge)\n",
    "    # tuple(sorted(c) for c in next(comp))\n",
    "\n",
    "    node_groups = []\n",
    "    for com in next(communities):\n",
    "        node_groups.append(list(com))\n",
    "    node_groupslen = [len(group) for group in node_groups]\n",
    "    communitydf = pd.DataFrame({\n",
    "    'communityID':np.arange(0, len(node_groupslen)),\n",
    "    'nodegroup':node_groups,\n",
    "    'groupsize':node_groupslen\n",
    "    })\n",
    "    return communitydf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b105370-d4de-4d6d-9e1e-f5b1c21ed419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_confirm_group(communitydf, DB, example_com):\n",
    "    \"\"\"This function only identify the exactly same group, disregarding groups appear across frames.\n",
    "    for example, track 1, 2 in frame 3-8, track 1,2,3 in frame 7-9\n",
    "    \"\"\"\n",
    "    tracks = communitydf[communitydf[\"communityID\"]==example_com][\"nodegroup\"].values[0]\n",
    "    tracks = [int(x) for x in tracks]\n",
    "    tracks.sort()\n",
    "    tracksls = \"_\".join([str(x) for x in tracks])\n",
    "\n",
    "    # for each frame and social id, get a list of tracks\n",
    "    sel = DB[(DB[\"communityID\"]==example_com)&(DB[\"track_id\"].isin(tracks))&(DB[\"Social\"]!=-1)]\n",
    "    frame_summary = sel.groupby([\"frame_id\", \"Social\"])[\"track_id\"].unique().reset_index()\n",
    "    frame_summary[\"track_id_str\"] = frame_summary[\"track_id\"].apply(lambda x: \"_\".join([str(i) for i in x]))\n",
    "    confirmed = frame_summary[frame_summary[\"track_id_str\"]==tracksls]\n",
    "    confirmed[\"communityID\"] = example_com\n",
    "    return confirmed\n",
    "\n",
    "\n",
    "def get_confirm_group_loose(communitydf, DB, example_com):\n",
    "\n",
    "    tracks = communitydf[communitydf[\"communityID\"]==example_com][\"nodegroup\"].values[0]\n",
    "    tracks = [int(x) for x in tracks]\n",
    "    tracks.sort()\n",
    "    tracksls = \"_\".join([str(x) for x in tracks])\n",
    "\n",
    "    # for each frame and social id, get a list of tracks\n",
    "    sel = DB[(DB[\"communityID\"]==example_com)&(DB[\"track_id\"].isin(tracks))&(DB[\"Social\"]!=-1)]\n",
    "    frame_summary = sel.groupby([\"frame_id\", \"Social\"])[\"track_id\"].unique().reset_index()\n",
    "    frame_summary[\"track_id_str\"] = frame_summary[\"track_id\"].apply(lambda x: \"_\".join([str(i) for i in x]))\n",
    "    frame_summary[\"len\"] = frame_summary[\"track_id\"].apply(lambda x: len(x))\n",
    "    def get_inter(temp):\n",
    "        lst2 = temp[\"track_id_str\"].values[0].split(\"_\")\n",
    "        lst2 = [int(x) for x in temp[\"track_id_str\"].values[0].split(\"_\")]\n",
    "        intersection = [value for value in lst2 if value in tracks]\n",
    "        temp[\"track_ls_intersection\"] = \"_\".join([str(x) for x in intersection])\n",
    "        return temp\n",
    "\n",
    "    reconstruct = frame_summary[frame_summary[\"len\"]>1].groupby(\"track_id_str\").apply(get_inter)\n",
    "    reconstruct[\"communityID\"] = example_com\n",
    "    return reconstruct\n",
    "\n",
    "\n",
    "def get_group(communitydf, DB):\n",
    "    group_df = []\n",
    "    for comID in tqdm(communitydf[\"communityID\"].unique()):\n",
    "        temp = get_confirm_group(communitydf, DB,comID)\n",
    "        group_df.append(temp)\n",
    "\n",
    "    group_df = pd.concat(group_df).reset_index(drop = True)\n",
    "    return group_df\n",
    "\n",
    "def get_group_loose(communitydf, DB):\n",
    "    group_df = []\n",
    "    for comID in tqdm(communitydf[\"communityID\"].unique()):\n",
    "        temp = get_confirm_group_loose(communitydf, DB, comID)\n",
    "        group_df.append(temp)\n",
    "\n",
    "    group_df = pd.concat(group_df).reset_index(drop = True)\n",
    "    return group_df\n",
    "\n",
    "def get_gender_comp(genderls):\n",
    "    if len(genderls)==2:\n",
    "        return \"Mixed\"\n",
    "    else:\n",
    "        return genderls[0]\n",
    "    \n",
    "    \n",
    "def valid_link_corr(DBSocial, x,y, thred = 0.5, n = 1):\n",
    "    samplegroup = DBSocial[DBSocial['track_id'].isin([x,y])]\n",
    "    # calculate the speed_x, speed_y correlation between track 10 and 11\n",
    "    df_wide = samplegroup.pivot(index = 'frame_id', \n",
    "                                columns = 'track_id', \n",
    "                                values = [f'speed_x_{n}s', \n",
    "                                          f'speed_y_{n}s',\n",
    "                                          f'speed_{n}s']).reset_index()\n",
    "    # calculate the correlation\n",
    "    df_wide = df_wide.dropna()\n",
    "    \n",
    "    coor1 = df_wide[(f\"speed_x_{n}s\",x)].corr(df_wide[(f\"speed_x_{n}s\",y)])\n",
    "    coor2 = df_wide[(f\"speed_y_{n}s\",x)].corr(df_wide[(f\"speed_y_{n}s\",y)])\n",
    "    coor3 = df_wide[(f\"speed_{n}s\",x)].corr(df_wide[(f\"speed_{n}s\",y)])\n",
    "    # corr4 is a stay indicator, if both speed_{n}s <0.5, thenn it is a stay\n",
    "    speed_mean1 = df_wide[(f\"speed_{n}s\",x)].mean()\n",
    "    speed_mean2 = df_wide[(f\"speed_{n}s\",y)].mean()\n",
    "    if speed_mean1<0.5 and speed_mean2<0.5:\n",
    "        coor4 = 1\n",
    "    else:\n",
    "        coor4 = 0\n",
    "    return coor1, coor2, coor3, coor4\n",
    "\n",
    "def get_selfile(framsel, thre=2):\n",
    "\n",
    "    seldb = DBcluster[DBcluster['frame_id']==framsel].reset_index(drop = True)\n",
    "    seldb = gpd.GeoDataFrame(seldb, geometry=[Point(x,y) for x,y in zip(seldb['lon'], seldb['lat'])], crs = f\"EPSG:4326\")\n",
    "    ax = seldb[seldb['Social']!=-1].plot(column = 'Social', legend = True, cmap = 'tab20', figsize = (8,8))\n",
    "    seldb[seldb['Social']==-1].plot(color = 'grey', ax = ax)\n",
    "    \n",
    "    seldb_shift = DBcluster[DBcluster['frame_id']==framsel+thre*2].reset_index(drop = True)\n",
    "    seldb_shift = gpd.GeoDataFrame(seldb_shift, geometry=[Point(x,y) for x,y in zip(seldb_shift['lon'], \n",
    "                                                                                    seldb_shift['lat'])], \n",
    "                                   crs = f\"EPSG:4326\")\n",
    "    ax = seldb_shift[seldb_shift['Social']!=-1].plot(column = 'Social', legend = True, cmap = 'tab20', figsize = (8,8))\n",
    "    seldb_shift[seldb_shift['Social']==-1].plot(color = 'grey', ax = ax)\n",
    "    return seldb, seldb_shift\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9215a277",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69aa771e",
   "metadata": {},
   "source": [
    "# Experiment, add speed similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE ARE CONSTANT VARIABLES\n",
    "fps = 29.9\n",
    "\n",
    "selp = 'social'\n",
    "# selp = 'personal_far'\n",
    "iditem = 'group_id_{}'.format(selp)\n",
    "timethred = 1\n",
    "\n",
    "metadata = {\n",
    "    'order':\"video order in one location\", \n",
    "     'video_location':\"video location name\", \n",
    "     'track_id':\"reconstructed track id, unique within each video\", \n",
    "       'video_id':\"video id, unique within each location\",\n",
    "       'lat':\"prejected latitude\",\n",
    "       'lon':\"prejected longitude\",\n",
    "       'track_id_backup':\"original track id from the tracking file\", \n",
    "       'speed_0.5s':\"speed in meter per second\",\n",
    "       'speed_x_0.5s':\"speed in meter per second in x direction\", \n",
    "       'speed_y_0.5s':\"speed in meter per second in y direction\", \n",
    "       'hex_id':\"h3 level 15 index\", \n",
    "       'inside':\"inside the comparable area (both historical and current) or not\",\n",
    "        'frame_id':\"reconstructed frame_id, across videos in a location, unique within one location\", \n",
    "        'frame_id_original':\"original frame_id from the tracking file\", \n",
    "        'second_from_start':\"calculated second from start based on the frame_id, 48 frames per real second\",\n",
    "       'appear_sec':\"total second the track appeared in the video\", \n",
    "       'individual_frame_total':\"total number of frames the track appeared in the video\", \n",
    "       'Social':\"spatial cluster id, unique within each frame, disregarding invalid or valid across time\", \n",
    "       'frame_social_track':\"frame_id + Social + track_id\",\n",
    "       'group_id_social':\"frame_id + Social, unique within each video\",\n",
    "       'group_size':\"number of tracks in the group\",\n",
    "       'is_group':\"whether the track is in a group or not\",\n",
    "       'group_first_frame':\"first frame_id when the track is in a group\",\n",
    "       'track_first_frame':\"first frame_id when the track appear in this video\", \n",
    "       'group_track_delta':\"difference between group_first_frame and track_first_frame\", \n",
    "       'emerging_group':\"whether the group is newly formed or not\",\n",
    "       'cross_frame_group_id':\\\n",
    "         \"this is a group id that can be used to identify the group across frames (only available for current videos)\",\n",
    "         \"gender\":\"gender of each pedestrian\",\n",
    "         \"age\":\"age of each pedestrian\",\n",
    "         'timestamp':\"timestamp of each frame (Only available for modern videos). use for reference.\"\n",
    "}\n",
    "selcols = list(metadata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872afc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fist_stagingfolder = \"../../_data/05_tracking_result_projected/step1_speed_vector\"\n",
    "stagingfolder = \"../../_data/05_tracking_result_projected/step1_speed_vector/current\"\n",
    "# resultfolder = '../../_data/10_clean/03_individual/current/20230711c/'\n",
    "resultfolder = '../../_data/10_clean/03_individual/current/20230711d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c048ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allvideos = glob.glob(os.path.join(fist_stagingfolder, \"*.csv\"))\n",
    "# allvideonames = [os.path.basename(x) for x in allvideos]\n",
    "# remain = [x for x in allvideonames if x not in finished]\n",
    "# len(remain)\n",
    "\n",
    "# videogroup = set([x.split(\"b\")[0] for x in remain])\n",
    "videogroup = [\n",
    "    '20081008-072238',\n",
    " '20100519-083343',\n",
    " '20100521-074701',\n",
    " '20100612-112553',]\n",
    "sel = ['Met20100612-120118', \n",
    "       'bryant_park20081008-141944',\n",
    "       'Downtown20100521-115755',\n",
    "       'Chestnut20100519-083343',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e3f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathdf['video_group'] = pathdf['video_group_name'].apply(lambda x: x[-15:])\n",
    "# pathdf['video_group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d9e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_method(traceGDF, DBSocial, DBcluster,df_links_valid, videoname, interpolation = True):\n",
    "    data_link = DBSocial.groupby(['frame_id', 'Social'])['track_id'].unique().reset_index()\n",
    "    data_link['group_member'] = data_link.apply(lambda x: \"&&\".join([str(i) for i in x['track_id']]), axis = 1)\n",
    "\n",
    "    #measure group length\n",
    "    data_link['group_len'] = data_link['track_id'].apply(lambda x: len(x))\n",
    "    # make a list of all possible combination of 2 people\n",
    "    data_link['combination2'] = data_link['track_id'].apply(lambda x: list(combinations(x, 2)))\n",
    "    data_link_explode = data_link[['frame_id',  'Social',\n",
    "        'combination2']].explode('combination2').reset_index(drop = True)\n",
    "    data_link_explode['u_v'] = data_link_explode['combination2'].apply(lambda x: \"&&\".join([str(i) for i in x]))\n",
    "    data_link_explode['v_u'] = data_link_explode['combination2'].apply(lambda x: \"&&\".join([str(i) for i in x[::-1]]))\n",
    "\n",
    "    df_links_valid['u_v'] = df_links_valid.apply(lambda x: \"&&\".join([str(x['u']), str(x['v'])]), axis = 1)\n",
    "    df_links_valid['v_u'] = df_links_valid.apply(lambda x: \"&&\".join([str(x['v']), str(x['u'])]), axis = 1)\n",
    "    # check if the u_v combination exist in the valid link dataframe\n",
    "    demolinks = data_link_explode[(data_link_explode['u_v'].isin(df_links_valid['u_v'].unique()))|(data_link_explode['u_v']\\\n",
    "        .isin(df_links_valid['v_u'].unique()))\n",
    "        ].reset_index(drop = True)\n",
    "    # flatten the groups to each track\n",
    "    demolinks = demolinks[['frame_id', \n",
    "                        'Social', \n",
    "                        'combination2']].explode('combination2')\\\n",
    "    .reset_index(drop = True).sort_values(['frame_id', 'Social'], ascending=False).rename(columns = {'combination2':'track_id'})\\\n",
    "        .drop_duplicates(['frame_id', 'Social', 'track_id']).reset_index(drop = True)\n",
    "    demolinks['frame_social_track'] = demolinks['frame_id'].astype(str)+\"$$\"+\\\n",
    "        demolinks['Social'].astype(str)+\"$$\"+demolinks['track_id'].astype(str)\n",
    "        \n",
    "    # confirm the data can be merged back to the original data\n",
    "    DBSocial['frame_social_track'] = DBSocial['frame_id'].astype(str)+ \"$$\"\\\n",
    "        + DBSocial['Social'].astype(str)+\"$$\"\\\n",
    "            +DBSocial['track_id'].astype(str)\n",
    "    DBSocial_update = DBSocial[DBSocial['frame_social_track'].isin(demolinks['frame_social_track'].unique())]\\\n",
    "        .reset_index(drop = True)\n",
    "    per = DBSocial_update.shape[0]/DBcluster.shape[0] # 26% observation ever in a group\n",
    "    # Social cluster id become the group id within each frame\n",
    "    DBSocial_update['group_id_social'] = DBSocial_update['frame_id'].astype(str) + '_' +DBSocial_update['Social'].astype(str)\n",
    "\n",
    "    # create the True Group ID\n",
    "    DBSocial_group = DBSocial_update.groupby(['frame_id', 'Social'])['track_id'].unique().reset_index()\n",
    "    DBSocial_group['truegroup'] = DBSocial_group['track_id'].apply(lambda x: \"&&\".join([str(i) for i in x]))\n",
    "\n",
    "\n",
    "\n",
    "    DBSocial_update = DBSocial_update[[ 'group_id_social','frame_id', 'Social','track_id']]\\\n",
    "                                        .merge(DBSocial_group[['frame_id', 'Social', 'truegroup']], \n",
    "                                                on = ['frame_id', 'Social'], how = 'inner')\n",
    "    # for each track, if the frame_id within its group first and last frame, then it is a group\n",
    "    def get_largergroup(DBSocial_group, DBcluster):\n",
    "        DBsocial_group_update = []\n",
    "        for truegroup in DBSocial_group['truegroup'].unique():\n",
    "            temp = DBSocial_update[DBSocial_update['truegroup']==truegroup].reset_index(drop = True)\n",
    "            trackls = temp['track_id'].unique()\n",
    "            firstframe = temp['frame_id'].min()\n",
    "            lastframe = temp['frame_id'].max()\n",
    "            # print(temp.shape[0])}\n",
    "            allvalid = DBcluster[(DBcluster['track_id'].isin(trackls))&(DBcluster['frame_id']<=lastframe)&(DBcluster['frame_id']>=firstframe)]\n",
    "            allvalid['truegroup'] = truegroup\n",
    "            # print(allvalid.shape[0])\n",
    "            DBsocial_group_update.append(allvalid)\n",
    "        DBsocial_group_update = pd.concat(DBsocial_group_update).reset_index(drop = True)\n",
    "        return DBsocial_group_update\n",
    "    if interpolation == True:\n",
    "        DBsocial_group_update = get_largergroup(DBSocial_group, DBcluster)\n",
    "    else:\n",
    "        DBsocial_group_update = DBSocial_group.copy()\n",
    "    # NOW WE MAY HAVE DUPLICATES>> MERGE THEM >> EACH TRACK SHOULD ONLY HAVE ONE TRUEGROUP IN ONE FRAME\n",
    "    \n",
    "    # DBsocial_group_update = DBSocial_update.copy()\n",
    "\n",
    "    DBsocial_group_update['group_id_social'] = DBsocial_group_update['frame_id'].astype(str) + '_' +DBsocial_group_update['Social'].astype(str)\n",
    "    DBsocial_group_update['frame_social_track'] = DBsocial_group_update['frame_id'].astype(str)+ \"$$\"\\\n",
    "        +DBsocial_group_update['Social'].astype(str)+\"$$\"\\\n",
    "            +DBsocial_group_update['track_id'].astype(str)\n",
    "    DBsocial_group_update = DBsocial_group_update.drop_duplicates(['frame_id', 'track_id'])\n",
    "    \n",
    "    DBcluster.drop('group_id_social', axis = 1, inplace = True)\n",
    "    \n",
    "    DBcluster['frame_social_track'] = DBcluster['frame_id'].astype(str)+ \"$$\"\\\n",
    "        +DBcluster['Social'].astype(str)+\"$$\"\\\n",
    "            +DBcluster['track_id'].astype(str)\n",
    "    # DBcluster.drop('group_id_social', axis = 1, inplace = True)\n",
    "    DBcluster_update = DBcluster.merge(DBsocial_group_update[['frame_social_track', 'truegroup','group_id_social']],\n",
    "                                            on = 'frame_social_track', how = 'left')\n",
    "    # merge the DBSocial_update back to the DBcluster\n",
    "    DBcluster_update['is_group'] = np.where(DBcluster_update['group_id_social'].isnull(), False, True)\n",
    "\n",
    "    # check if a group is newly formed or not\n",
    "    # for each track, find its first frame_id when it is in a group\n",
    "    DBcluster_update['group_first_frame'] = DBcluster_update.groupby(['track_id','is_group'])['frame_id'].transform('min')\n",
    "    DBcluster_update['group_last_frame'] = DBcluster_update.groupby(['track_id','is_group'])['frame_id'].transform('max')\n",
    "    DBcluster_update['group_first_frame'] = np.where(DBcluster_update['is_group']==False, \n",
    "                                                    np.nan, DBcluster_update['group_first_frame'])\n",
    "    DBcluster_update['group_last_frame'] = np.where(DBcluster_update['is_group']==False, \n",
    "                                                    np.nan, DBcluster_update['group_last_frame'])\n",
    "\n",
    "\n",
    "    DBcluster_update['track_first_frame'] = DBcluster_update.groupby(['track_id'])['frame_id'].transform('min')\n",
    "    DBcluster_update['group_track_delta'] = DBcluster_update['group_first_frame'] - DBcluster_update['track_first_frame']\n",
    "    DBcluster_update['emerging_group'] = np.where(DBcluster_update['group_track_delta']>29.97*5, True, False) # 1 second\n",
    "    DBcluster_update = DBcluster_update.drop_duplicates(['track_id',  'frame_id'])\n",
    "\n",
    "    DBcluster_update['appear_sec'] = DBcluster_update.groupby('track_id')['frame_id'].transform('count')/fps\n",
    "    DBcluster_update['individual_frame_total'] = DBcluster_update.groupby('track_id')['frame_id'].transform('count')\n",
    "    DBcluster_update['group_size'] = DBcluster_update['truegroup'].fillna(\"\").apply(lambda x: len(x.split('&&')))\n",
    "    DBcluster_update.rename(columns = {\n",
    "            'truegroup':'cross_frame_group_id',\n",
    "            \n",
    "        }, inplace = True)\n",
    "    exportcols = [x for x in selcols if x in DBcluster_update.columns]\n",
    "    assert DBcluster_update.shape[0]==traceGDF.shape[0]\n",
    "    DBcluster_update[exportcols].to_csv(os.path.join(resultfolder, \n",
    "                                                            f\"{videoname}.csv\"), index = False)\n",
    "    return DBcluster_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f390d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# del fullgdf, traceGDF, DBSocial, DBcluster, df_links_valid, df_links, DBcluster_update\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [5,1,2,4]:\n",
    "    video_group_name = pathdf.loc[i,\"video_group_name\"]\n",
    "    video_loc = pathdf.loc[i, 'loc']\n",
    "    \n",
    "    fullgdf = pd.read_csv(os.path.join(stagingfolder, f\"{video_group_name}_full.csv\"), \n",
    "                          engine = \"python\").drop_duplicates(['track_id', 'frame_id'])\\\n",
    "                              .reset_index(drop = True)\n",
    "    videols = fullgdf['video_id'].unique()\n",
    "    for videoname in tqdm(videols):\n",
    "        traceGDF = fullgdf[fullgdf['video_id']==videoname].reset_index(drop = True)\n",
    "        DBSocial, DBcluster = generate_social(traceGDF, 3857, dis = 1.9)\n",
    "        df_links = getuvperframe(DBSocial, iditem)\n",
    "        df_links = df_links.groupby(['u', 'v']).size().reset_index().rename(columns={0: 'weight'})\n",
    "        df_links = df_links[df_links['weight'] > 1*fps].reset_index(drop = True)\n",
    "        df_links['coor_ls'] = df_links.apply(lambda x: valid_link_corr(DBSocial, x['u'], x['v'], n = 0.5),axis = 1)\n",
    "        df_links['valid'] = np.where(df_links['coor_ls'].apply(lambda x: x[0]>0.0 or x[1]>0.0), True, False)\n",
    "        # links are valid if all people in a group are staying\n",
    "        df_links['valid'] = np.where(df_links['coor_ls'].apply(lambda x: x[3]>0), True, df_links['valid'])\n",
    "        \n",
    "        df_links_valid = df_links[(df_links['valid']==True)].reset_index(drop = True) # 20230711c\n",
    "        # df_links_valid = df_links[(df_links['weight'] > 1*fps)].reset_index(drop = True) # 20230711b\n",
    "        DBcluster_update = link_method(traceGDF, DBSocial, DBcluster,df_links_valid, videoname, \n",
    "                                       interpolation = True)\n",
    "### for all valid links if not able to construct community, keep the valid link and memebers directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78979173",
   "metadata": {},
   "outputs": [],
   "source": [
    "DBcluster_update = link_method(traceGDF, DBSocial, DBcluster,df_links_valid, videoname, \n",
    "                                       interpolation = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2476f3fc",
   "metadata": {},
   "source": [
    "# Visualize the finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d0cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selfile(framsel, thre=2):\n",
    "\n",
    "    seldb = DBcluster[DBcluster['frame_id']==framsel].reset_index(drop = True)\n",
    "    seldb = gpd.GeoDataFrame(seldb, geometry=[Point(x,y) for x,y in zip(seldb['lon'], seldb['lat'])], crs = f\"EPSG:4326\")\n",
    "    ax = seldb[seldb['Social']!=-1].plot(column = 'Social', legend = True, cmap = 'tab20', figsize = (8,8))\n",
    "    seldb[seldb['Social']==-1].plot(color = 'grey', ax = ax)\n",
    "    \n",
    "    seldb_shift = DBcluster[DBcluster['frame_id']==framsel+thre*2].reset_index(drop = True)\n",
    "    seldb_shift = gpd.GeoDataFrame(seldb_shift, geometry=[Point(x,y) for x,y in zip(seldb_shift['lon'], \n",
    "                                                                                    seldb_shift['lat'])], \n",
    "                                   crs = f\"EPSG:4326\")\n",
    "    ax = seldb_shift[seldb_shift['Social']!=-1].plot(column = 'Social', legend = True, cmap = 'tab20', figsize = (8,8))\n",
    "    seldb_shift[seldb_shift['Social']==-1].plot(color = 'grey', ax = ax)\n",
    "    return seldb, seldb_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c494f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize groups identified in the MET video\n",
    "videogroup = '20100612-120118'\n",
    "individualfolder = '../../_data/10_clean/03_individual/'\n",
    "currentfolder = os.path.join(individualfolder, 'current')\n",
    "files = os.listdir(currentfolder)\n",
    "files = [x for x in files if videogroup in x]\n",
    "# videoname = '20100612-120118b01'\n",
    "df = pd.read_csv(os.path.join(currentfolder, videoname+'.csv')).drop_duplicates(['track_id', 'frame_id'])\n",
    "\n",
    "result_folder = \"/Users/yuan/Dropbox (MIT)/whyte_CV/_data/03_tracking_result/_current_video_no_attr\"\n",
    "predpath = os.path.join(result_folder, f'{videoname}.txt')\n",
    "trace = pd.read_csv(predpath, sep = '\\t', header = None)\n",
    "trace.columns = [ \"x1\", \"y1\", \"x2\", \"y2\", \"track_id\", \"frame_id\"]\n",
    "trace['w'] = trace['x2'] - trace['x1']\n",
    "trace['h'] = trace['y2'] - trace['y1']\n",
    "trace['ratio'] = trace['w']/trace['h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2dd7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image and check\n",
    "selframe = 12583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fb64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_sel = 12583\n",
    "seldb = df[df['frame_id']==frame_sel]\n",
    "\n",
    "\n",
    "# df_sel.groupby('Social').size()\n",
    "data = trace.merge(seldb[[ \"frame_id\", 'Social','track_id','is_group']], \n",
    "                                               left_on = [\"track_id\", \"frame_id\"],\n",
    "                                               right_on = [\"track_id\", \"frame_id\"],\n",
    "                                               suffixes=('', '_new'))\n",
    "data = data[data['Social']!=-1].reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31053b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_color_for_labels(label):\n",
    "    \"\"\"\n",
    "    Simple function that adds fixed color depending on the class\n",
    "    \"\"\"\n",
    "    palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n",
    "    color = [int((p * (label ** 2 - label + 1)) % 255) for p in palette]\n",
    "    return tuple(color)\n",
    "\n",
    "def getbasics(file_path):\n",
    "    video = cv2.VideoCapture(file_path)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    length = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print('frames per second =',fps)\n",
    "    size = (int(video.get(cv2.CAP_PROP_FRAME_WIDTH)), int(video.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    print('frames size =',size)\n",
    "    # video.release()\n",
    "    return video, fps, size, length\n",
    "\n",
    "def plot_img(video_name, framsel, data):\n",
    "    video_file = videopath = f\"/Users/yuan/Dropbox (MIT)/whyte_CV/_data/00_raw/videos_current_highres/Met Steps videos (NEW)/20100521-115754-01/{video_name}.avi\"\n",
    "    \n",
    "    video, fps, size, length= getbasics(video_file)\n",
    "        # set video to the frame\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, framsel)\n",
    "    re, frame = video.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    for j in range(data.shape[0]):\n",
    "        social_id = data.at[j, 'Social']\n",
    "        color = compute_color_for_labels(social_id)\n",
    "        cv2.rectangle(frame,\n",
    "                        (data.at[j,'x1'], \n",
    "                    data.at[j,'y1']), \n",
    "                    (data.at[j,'x2'], \n",
    "                    data.at[j,'y2']), \n",
    "                        color, 2)\n",
    "\n",
    "    fig = plt.subplots(figsize = (10,10))\n",
    "    plt.imshow(frame)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ac3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(videoname, frame_sel, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(videoname, frame_sel, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27719b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the frame from video\n",
    "videopath = f\"/Users/yuan/Dropbox (MIT)/whyte_CV/_data/00_raw/videos_current_highres/Met Steps videos (NEW)/20100521-115754-01/{file}.avi\"\n",
    "def getbasics(file_path):\n",
    "    video = cv2.VideoCapture(file_path)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    length = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print('frames per second =',fps)\n",
    "    size = (int(video.get(cv2.CAP_PROP_FRAME_WIDTH)), int(video.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    print('frames size =',size)\n",
    "    # video.release()\n",
    "    return video, fps, size, length\n",
    "\n",
    "video, fps, size, length = getbasics(videopath)\n",
    "video.set(cv2.CAP_PROP_POS_FRAMES, frame_sel)\n",
    "frame = video.read()[1]\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "fig = plt.subplots(figsize = (10,10))\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90da48e8-119e-4f21-9372-98e1c9eecd29",
   "metadata": {},
   "source": [
    "# Summarize group attributes (archive below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8972f28-eef1-4742-8555-2606d86dc470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original prediction result\n",
    "# result_folder = \"../data/06_attr_result/\"\n",
    "# # predpath = os.path.join(result_folder, f'{videoname}_attr_mot.csv')\n",
    "# predpath = os.path.join(result_folder, f'{videoname}.csv')\n",
    "# trace = pd.read_csv(predpath)\n",
    "# trace.rename(columns = {\"x\":\"bbox0\", \"y\":\"bbox1\", \"w\":\"bbox2\", \"h\":\"bbox3\"}, inplace = True)\n",
    "# for x in [\"bbox0\", \"bbox1\", \"bbox2\", \"bbox3\"]:\n",
    "#     trace[x] = trace[x].astype(int)\n",
    "\n",
    "# for each track, get the major gender and age\n",
    "traceGDF[\"gender\"] = traceGDF.groupby(\"track_id\")[\"gender\"].transform(lambda x: x.mode()[0])\n",
    "traceGDF[\"track_id\"] = traceGDF[\"track_id\"].astype(int)\n",
    "\n",
    "\n",
    "seltrack = traceGDF.merge(group_df_l_flat, on = [\"frame_id\", \"track_id\"], how = \"left\")\n",
    "group_attr = group_df_l_flat[[\"truegroup\", \"group_first_frame\", \"group_last_frame\"]].drop_duplicates(\"truegroup\")\n",
    "\n",
    "# missing one step, fill the inner frames betwen the first and last frame\n",
    "# for any frames that within (min, max) and the N people are identified in the same frame, assign the same truegroup value\n",
    "newgroup = []\n",
    "for g in group_attr[\"truegroup\"].unique():\n",
    "    first = group_attr[group_attr[\"truegroup\"]==g][\"group_first_frame\"].values[0]\n",
    "    last = group_attr[group_attr[\"truegroup\"]==g][\"group_last_frame\"].values[0]\n",
    "    trackls = [int(x) for x in g.split(\"_\")]\n",
    "    temp = seltrack[(seltrack[\"frame_id\"]<=last)&(seltrack[\"frame_id\"]>=first)].reset_index(drop = True)\n",
    "    temp = temp[temp[\"track_id\"].isin(trackls)].reset_index(drop = True)\n",
    "    \n",
    "    temp[\"truegroup\"]  = temp[\"truegroup\"].fillna(method = \"ffill\")\n",
    "    newgroup.append(temp)\n",
    "    \n",
    "newgroup = pd.concat(newgroup).reset_index(drop = True)\n",
    "# use this for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e647d8-9dd7-4d17-95b5-951f7686a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "traceGDF.drop(['video_id'], \n",
    "              axis = 1, \n",
    "              inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e884266-6bb5-4057-9eb5-582f95be4b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge back to group lat lon\n",
    "grouptracedf = newgroup[['track_id', \n",
    "          'frame_id', \n",
    "        'Social', \n",
    "          'truegroup',\n",
    "       'group_first_frame', \n",
    "        'group_last_frame']].merge(\n",
    "    traceGDF,\n",
    "               on = [\"frame_id\", \"track_id\"],\n",
    "               how = \"right\"\n",
    "              )\n",
    "grouptracedf[\"individual_first_frame\"] = grouptracedf.groupby(\"track_id\")[\"frame_id\"].transform(\"min\")\n",
    "grouptracedf[\"individual_last_frame\"] = grouptracedf.groupby(\"track_id\")[\"frame_id\"].transform(\"max\")\n",
    "grouptracedf.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92772f8a",
   "metadata": {},
   "source": [
    "# NOT FINISHED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e4b25-f05b-495a-84ff-6c24adcc2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group size\n",
    "summary = newgroup.groupby(\"truegroup\")[\"track_id\"].nunique().reset_index().rename(columns = {\"track_id\":\"group_size\"})\n",
    "\n",
    "# group appearing time and walking speed\n",
    "grouptracegdf = gpd.GeoDataFrame(grouptracedf, geometry = [Point(x,y) for x,y in zip(grouptracedf[\"lon\"],\n",
    "                                                                                    grouptracedf[\"lat\"])])\n",
    "grouptracegdf.crs = \"EPSG:4326\"\n",
    "\n",
    "# select only groups and convert to lines\n",
    "group_gdf = grouptracegdf[~grouptracegdf[\"truegroup\"].isna()].reset_index(drop = True)\n",
    "single_gdf = grouptracegdf[grouptracegdf[\"truegroup\"].isna()].reset_index(drop = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d6ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouptracedf_update = grouptracedf.merge(summary, on = \"truegroup\", how = \"left\").drop_duplicates(['track_id',  'truegroup',\n",
    "#     'group_first_frame', 'group_last_frame']).reset_index(drop = True)\n",
    "# grouptracedf_update['delta_frame'] = grouptracedf_update['group_first_frame'] - grouptracedf_update['individual_first_frame']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0debc3-ad97-4a6a-a645-9f1b7e4124f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert point to linestrings\n",
    "from shapely.geometry import LineString\n",
    "group_gdf[\"indi_totalFrame\"] = group_gdf.groupby(\"track_id\")[\"frame_id\"].transform(\"count\")\n",
    "group_gdf = group_gdf[group_gdf[\"indi_totalFrame\"]>1].reset_index(drop = True)\n",
    "group_gdf_line = group_gdf.sort_values(\"frame_id\").reset_index(drop = True)\\\n",
    ".groupby(['track_id',\"gender\", \"age\",\"indi_totalFrame\",\"individual_last_frame\", \n",
    "          \"individual_first_frame\"])['geometry'].apply(lambda x: LineString(x.tolist())).reset_index()\n",
    "\n",
    "single_gdf[\"indi_totalFrame\"] = single_gdf.groupby(\"track_id\")[\"frame_id\"].transform(\"count\")\n",
    "single_gdf = single_gdf[single_gdf[\"indi_totalFrame\"]>1].reset_index(drop = True)\n",
    "\n",
    "single_gdf_line = single_gdf.sort_values(\"frame_id\").reset_index(drop = True)\\\n",
    ".groupby(['track_id',\"gender\", \"age\",\"indi_totalFrame\",\"individual_last_frame\", \"individual_first_frame\"])['geometry'].apply(lambda x: LineString(x.tolist())).reset_index()\n",
    "group_gdf_line.crs = \"EPSG:4326\"\n",
    "single_gdf_line.crs = \"EPSG:4326\"\n",
    "group_gdf_line = group_gdf_line.to_crs(f\"EPSG:{epsg}\")\n",
    "single_gdf_line = single_gdf_line.to_crs(f\"EPSG:{epsg}\")\n",
    "\n",
    "group_gdf_line[\"moving_distance\"] = group_gdf_line[\"geometry\"].length\n",
    "single_gdf_line[\"moving_distance\"] = single_gdf_line[\"geometry\"].length\n",
    "group_gdf_line[\"indi_sec\"] = group_gdf_line[\"indi_totalFrame\"]/fps\n",
    "single_gdf_line[\"indi_sec\"] = single_gdf_line[\"indi_totalFrame\"]/fps\n",
    "\n",
    "# this moving_speed is at individual average level\n",
    "group_gdf_line[\"moving_speed\"] = group_gdf_line[\"moving_distance\"]/group_gdf_line[\"indi_sec\"]\n",
    "single_gdf_line[\"moving_speed\"] = single_gdf_line[\"moving_distance\"]/single_gdf_line[\"indi_sec\"]\n",
    "\n",
    "group_gdf_line[\"track_id\"] = group_gdf_line[\"track_id\"].astype(int)\n",
    "group_attr1 = group_gdf[~group_gdf[\"group_first_frame\"].isna()][['track_id',  'truegroup',\n",
    "       'group_first_frame', 'group_last_frame']].drop_duplicates()\n",
    "\n",
    "group_gdf[\"track_id\"] = group_gdf[\"track_id\"].astype(int)\n",
    "\n",
    "group_attr2 = group_gdf_line.merge(\n",
    "    group_attr1,\n",
    "    on = \"track_id\"\n",
    ")\\\n",
    ".merge(summary, on = [\"truegroup\"])\n",
    "group_attr2[\"group_totalFrame\"] = group_attr2[\"group_last_frame\"] - group_attr2[\"group_first_frame\"]\n",
    "\n",
    "# calculate the time difference between individual's first appearing time and group appearing time\n",
    "group_attr2[\"delta_frame\"] = group_attr2[\"group_first_frame\"] - group_attr2[\"individual_first_frame\"] \n",
    "group_attr_valid = group_attr2[group_attr2[\"moving_speed\"]<7] # 3.6 ft/s - 6 ft/s # 2263 at ft, 3857 at meter\n",
    "\n",
    "\n",
    "group_attr_valid[\"gender_indi\"] = np.where(group_attr_valid[\"gender\"]==\"Male\",0, 1)\n",
    "gendersummary = group_attr_valid.drop_duplicates([\"truegroup\", \"track_id\"]).groupby(\"truegroup\")[\"gender\"].unique().reset_index()\n",
    "gendersummary[\"gender_comp\"] = gendersummary[\"gender\"].apply(get_gender_comp)\n",
    "gendergroup = gendersummary.groupby(\"gender_comp\").size().reset_index().rename(columns = {0:\"count\"})\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6eea41d-8981-4412-94ce-3dacbe9b1edf",
   "metadata": {},
   "source": [
    "## Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21bd14a-4714-4f05-9ac6-9e5a1859f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual analysis, use overall data per place to generate\n",
    "gendersummary = traceGDF.groupby(\"gender\")[\"track_id\"].nunique().reset_index()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "sns.barplot(\n",
    "    data = gendersummary,\n",
    "    x = \"gender\", \n",
    "    y = \"track_id\",\n",
    "    palette = [\"#ef5c43\", \"#3bc0cf\"]\n",
    "    \n",
    ")\n",
    "sns.despine()\n",
    "ax.set_xlabel(\"Gender\")\n",
    "ax.set_ylabel(\"Number of pedestrians\")\n",
    "ax.grid(axis = \"y\", color = \"grey\", linestyle = \"--\", linewidth = 0.5)\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_tick_params(rotation=0)\n",
    "plt.xticks(fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439e9571-d3f5-4ff5-8e0c-9f070b4e9d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group level gender comparison\n",
    "gendersummary = group_attr_valid.drop_duplicates([\"truegroup\", \"track_id\"]).groupby(\"truegroup\")[\"gender\"].unique().reset_index()\n",
    "gendersummary[\"gender_comp\"] = gendersummary[\"gender\"].apply(get_gender_comp)\n",
    "gendergroup = gendersummary.groupby(\"gender_comp\").size().reset_index().rename(columns = {0:\"count\"})\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "sns.barplot(\n",
    "    data = gendergroup,\n",
    "    x = \"gender_comp\", \n",
    "    y = \"count\",\n",
    "    palette = [\"#ef5c43\", \"#3bc0cf\",\"#ffdda6\"]\n",
    ")\n",
    "sns.despine()\n",
    "ax.set_xlabel(\"Gender Composition\")\n",
    "ax.set_ylabel(\"Number of observed groups\")\n",
    "ax.grid(axis = \"y\", color = \"grey\", linestyle = \"--\", linewidth = 0.5)\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_tick_params(rotation=0)\n",
    "plt.xticks(fontsize = 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "082bf8eb-c922-41e6-8549-d234269dff33",
   "metadata": {},
   "source": [
    "### Emerging interaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_valid = single_gdf_line[single_gdf_line[\"moving_speed\"]<3]\n",
    "single_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_emerging_thred(group_attr_valid, thred = 5):\n",
    "    group_attr_valid[\"if_emerging\"]= np.where(group_attr_valid[\"delta_frame\"]<thred*29.97,0, 1)\n",
    "    summary = group_attr_valid.groupby(\"if_emerging\")[\"track_id\"].nunique().reset_index()\n",
    "    summary[\"thred\"] = thred\n",
    "    return summary, group_attr_valid\n",
    "\n",
    "summary, group_attr_valid = set_emerging_thred(group_attr_valid)\n",
    "single_valid = single_gdf_line[single_gdf_line[\"moving_speed\"]<3]\n",
    "group_attr_valid\n",
    "# combine the single and group in one df and plot relationship\n",
    "single_valid[\"in_group\"] = 0\n",
    "single_valid[\"group_size\"] = 1\n",
    "group_attr_valid[\"in_group\"] = 1\n",
    "df_clean = pd.concat([single_valid, group_attr_valid]).reset_index(drop = True)\n",
    "\n",
    "# resultfolder = \"../data/05_tracking_result_projected/step4_full\"\n",
    "# df_clean.to_parquet(os.path.join(resultfolder, f\"{videoname}.parquet\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9befb3f-9f88-47cc-8a56-281b49673595",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# robust step\n",
    "summaryDF = []\n",
    "for thred in range(2, 30):\n",
    "    summary, group_attr_valid = set_emerging_thred(group_attr_valid, thred)\n",
    "    summaryDF.append(summary)\n",
    "summaryDF = pd.concat(summaryDF).reset_index(drop = True).pivot(columns = \"if_emerging\", values = \"track_id\", index = \"thred\")\\\n",
    ".rename(columns = {0:\"existing\", 1:\"emerging\"})\n",
    "summaryDF[\"total\"] = summaryDF[\"existing\"]+summaryDF[\"emerging\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce521f-40cb-412d-b610-2cddbeeb921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot the changes\n",
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "summaryDF[[\"existing\", \"emerging\"]].plot.bar(\n",
    "    stacked = True,\n",
    "    ax = ax,\n",
    "    color = [ \"#ffdda6\", \"#3bc0cf\"],\n",
    ")\n",
    "sns.despine()\n",
    "ax.set_xlabel(\"Threshold (seconds)\")\n",
    "ax.set_ylabel(\"Number of pedestrians\")\n",
    "ax.grid(axis = \"y\", color = \"grey\", linestyle = \"--\", linewidth = 0.5)\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_tick_params(rotation=0)\n",
    "# handles, labels = ax.get_legend_handles_labels()\n",
    "# ax.legend(handles, [\"Existing\", \"Emerging\"], title = \"Group Type\")\n",
    "ax.legend(bbox_to_anchor=(1, 1), loc='upper right', borderaxespad=0., fontsize = 10)\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c024433",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultfolder = \"../data/05_tracking_result_projected/step4_full\"\n",
    "# df_clean.to_csv(os.path.join(resultfolder, f\"{videoname}.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7452ba8-6afe-4115-b91b-a29c4f3be4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "sns.pointplot(\n",
    "    data = df_clean,\n",
    "    x = \"group_size\",\n",
    "    y = \"moving_speed\",\n",
    "    ci = 98,\n",
    "    errwidth = 1,\n",
    "    capsize=.2,\n",
    "    color = \"#3bc0cf\"\n",
    "\n",
    ")\n",
    "# sns.regplot(\n",
    "#     data = group_attr_valid,\n",
    "#     x = \"group_size\",\n",
    "#     y = \"moving_speed\",\n",
    "#     ax = ax,\n",
    "#     color = \"#3bc0cf\"\n",
    "# )\n",
    "ax.set_ylabel(\"Moving Speed (ft/s)\")\n",
    "ax.set_xlabel(\"Group Size\")\n",
    "sns.despine()\n",
    "\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccb349-ec6c-471b-aae5-60f8795bad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "df_clean[\"in_group_des\"] = np.where(df_clean[\"in_group\"]==0, \"Alone\", \"In Group\")\n",
    "\n",
    "daynightcolor = [\"#ef5c43\",\"#0c7cba\"]\n",
    "sns.set_palette(sns.color_palette(daynightcolor))\n",
    "\n",
    "sns.boxplot(\n",
    "    data = df_clean,\n",
    "    x = \"in_group_des\",\n",
    "    y = \"moving_speed\",\n",
    "    linewidth = 0.5\n",
    "    \n",
    ")\n",
    "ax.set_ylabel(\"Moving Speed (ft/s)\")\n",
    "ax.set_xlabel(\"Group Size\")\n",
    "sns.despine()\n",
    "\n",
    "\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c002b36-c556-417e-9056-7d6a042ff791",
   "metadata": {},
   "source": [
    "# Create Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85916eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "newgroupunique = newgroup.drop_duplicates([\"track_id\",\"frame_id\"]) \n",
    "newgroupunique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d1e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = \"../data/06_attr_result/\"\n",
    "# predpath = os.path.join(result_folder, f'{videoname}_attr_mot.csv')\n",
    "predpath = os.path.join(result_folder, f'{videoname}.csv')\n",
    "trace = pd.read_csv(predpath)\n",
    "trace.rename(columns = {\"x\":\"bbox0\", \"y\":\"bbox1\", \"w\":\"bbox2\", \"h\":\"bbox3\"}, inplace = True)\n",
    "for x in [\"bbox0\", \"bbox1\", \"bbox2\", \"bbox3\"]:\n",
    "    trace[x] = trace[x].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007eece0-034b-42b4-91e7-3387c2eaae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = \"../data/06_attr_result/\"\n",
    "# predpath = os.path.join(result_folder, f'{videoname}_attr_mot.csv')\n",
    "predpath = os.path.join(result_folder, f'{videoname}.csv')\n",
    "trace = pd.read_csv(predpath)\n",
    "trace.rename(columns = {\"x\":\"bbox0\", \"y\":\"bbox1\", \"w\":\"bbox2\", \"h\":\"bbox3\"}, inplace = True)\n",
    "for x in [\"bbox0\", \"bbox1\", \"bbox2\", \"bbox3\"]:\n",
    "    trace[x] = trace[x].astype(int)\n",
    "# Visualize by comparing the group activity and single activity\n",
    "def getbasics(file_path):\n",
    "    video = cv2.VideoCapture(file_path)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    print('frames per second =',fps)\n",
    "    size = (int(video.get(cv2.CAP_PROP_FRAME_WIDTH)), int(video.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    print('frames size =',size)\n",
    "    # video.release()\n",
    "    return video, fps, size\n",
    "\n",
    "clipfolder = \"../data/99_result_sample_export2\"\n",
    "if not os.path.exists(clipfolder):\n",
    "    os.makedirs(clipfolder)\n",
    "    \n",
    "file_path = video_df[video_df[\"video_id\"]==videoname][\"video_path\"].values[0]\n",
    "video, fps, size = getbasics(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df[video_df[\"video_id\"]==videoname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea83bd-4f72-4136-9428-d4843f895a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample setup\n",
    "dur = 120 # second, change here for other options\n",
    "# startframe = int(newgroup[\"frame_id\"].min())\n",
    "startframe = 1\n",
    "endframe = int(startframe + dur * fps)\n",
    "# endframe = int(newgroup[\"frame_id\"].max()) # fullvideo export\n",
    "\n",
    "# clip the data\n",
    "\n",
    "vizdata = newgroup[(newgroup[\"frame_id\"]<=endframe)&(newgroup[\"frame_id\"]>=startframe)].reset_index(drop = True)\n",
    "\n",
    "vizdata = vizdata.merge(trace[['track_id', 'frame_id',  'bbox0', 'bbox1', 'bbox2', 'bbox3']],\n",
    "                        on = ['track_id', 'frame_id'], how = 'inner')\n",
    "\n",
    "# set up color\n",
    "groups = vizdata['truegroup'].unique()\n",
    "colorls = [tuple(random.choices(range(i%255,255), k =3)) for i in range(len(groups))]\n",
    "colorset = dict(zip(groups, colorls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c17e3-aa76-42b3-9dae-395e292706e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set startframe\n",
    "video.set(cv2.CAP_PROP_POS_FRAMES, startframe)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "video_out_name = videoname + f\"_{startframe}_{endframe}_group\"\n",
    "video_out = cv2.VideoWriter(os.path.join(clipfolder, \n",
    "                                         f\"{video_out_name}.mp4\"), fourcc, fps, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b385569-c770-4f1d-be0b-dd28ba502209",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = True\n",
    "count = startframe\n",
    "frame_total = endframe - startframe\n",
    "\n",
    "while ret and count<=frame_total+startframe:\n",
    "    ret, frame = video.read()\n",
    "    frame_id = count\n",
    "    data = vizdata[vizdata[\"frame_id\"]==frame_id].reset_index(drop = True)\n",
    "    for j, tr in enumerate(data['track_id'].values):\n",
    "        gender = data.at[j,\"gender\"]\n",
    "        gr = data.at[j, 'truegroup']\n",
    "        # only viz the groups\n",
    "        if gr in groups:\n",
    "            cv2.rectangle(frame, \n",
    "                      (data.at[j,'bbox0'], \n",
    "                       data.at[j,'bbox1']), \n",
    "                      (data.at[j,'bbox0']+data.at[j,'bbox2'], \n",
    "                       data.at[j,'bbox1']+data.at[j,'bbox3']), \n",
    "                          colorset[gr], 2)\n",
    "            cv2.putText(\n",
    "                    frame, \n",
    "                    str(gr+\" \"+ gender),\n",
    "                    (data.at[j,'bbox0'], data.at[j,'bbox1']-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, colorset[gr],\n",
    "                    2,\n",
    "                    lineType=cv2.LINE_AA\n",
    "                )\n",
    "    # print(frame, ret)\n",
    "    video_out.write(frame)\n",
    "    count = count+1\n",
    "    # print(ret)\n",
    "video_out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6a82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c624f5cd4f43ba04980739061df70c70943b2041a65fc94f85b2846fd2e789f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
