{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h3 import h3\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from osgeo import gdal\n",
    "from osgeo import osr\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, LineString\n",
    "def imshow(image, show_axes = False, quiet = False):\n",
    "    if len(image.shape) == 3:\n",
    "      # Height, width, channels\n",
    "      # Assume BGR, do a conversion since \n",
    "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "      # Height, width - must be grayscale\n",
    "      # convert to RGB, since matplotlib will plot in a weird colormap (instead of black = 0, white = 1)\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    # Draw the image\n",
    "    plt.imshow(image)\n",
    "    if not show_axes:\n",
    "        # We'll also disable drawing the axes and tick marks in the plot, since it's actually an image\n",
    "        plt.axis('off')\n",
    "    if not quiet:\n",
    "        # Make sure it outputs\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal>> modern videos' h3 aggregation\n",
    "1. Plot a timeseries chart of observed people\n",
    "2. Aggregate to ground level per frame per h3 (count of people)\n",
    "`sum(count_of_people_per_frame)/total_frames`\n",
    "3. Table of total observed people, frame, year, h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selvideogroups = [\n",
    "    'Chestnut20100519-083343', \n",
    "       'Downtown20100521-115755',\n",
    "       'Met20100612-120118',\n",
    "       'bryant_park20081008-141944']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "outputfolder = \"../../_data/05_tracking_result_projected/step0_attr_prj\"\n",
    "frame_folder = \"../../_data/02_siteplan/sample_frames/current_sample\"\n",
    "tiff_folder = \"../../_data/02_siteplan/geo_tiff\"\n",
    "ref_folder = \"../../_data/02_siteplan/gcp_pt/\"\n",
    "# export data by location\n",
    "staginging_folder2 = \"../../_data/05_tracking_result_projected/step1_speed_vector/current\"\n",
    "if not os.path.exists(staginging_folder2):\n",
    "    os.mkdir(staginging_folder2)\n",
    "frames = os.listdir(frame_folder)\n",
    "refs = os.listdir(ref_folder)\n",
    "\n",
    "# each location can have two videos shoot in the morning or noon\n",
    "timedict = {\n",
    "    \"Chestnut20100519-083343\":\"morning\",\n",
    "    'Downtown20100521-074701':\"morning\",\n",
    "       'Downtown20100521-115755':\"noon\",\n",
    "       'Met20100612-082221':\"morning\",\n",
    "       'Met20100612-112553':'noon',\n",
    "       'Met20100612-120118':\"noon\", \n",
    "       'bryant_park20081008-072238':\"morning\",\n",
    "       'bryant_park20081008-141944':\"noon\",\n",
    "}\n",
    "\n",
    "def get_frame_num(time_str, fps = 29.97002997002997):\n",
    "    try:\n",
    "        time = time_str.split(\" \")[0][3:].split(\":\")\n",
    "        minute = int(time[0])\n",
    "        second = int(time[1])\n",
    "        frame = minute*60*fps + second*fps\n",
    "        return int(frame)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videopath = pd.read_csv(\"../../_data/00_raw/_video_meta/video_path_0509.csv\")\n",
    "videopath['video_id'] = videopath['video_name'].apply(lambda x: x.split(\".\")[0])\n",
    "videopath['video_group_update'] = videopath['video_location'].apply(lambda x: x.split(\" \")[0])+videopath['video_group']\n",
    "\n",
    "# videopath_sel = videopath[videopath['scene'].isin([2,3])]\n",
    "videopath_sel = videopath[~videopath['ref_path'].isna()].reset_index(drop = True)\n",
    "videopath_sel['first_effective_time'].unique()\n",
    "videopath_sel['first_effective_time'] = videopath_sel['first_effective_time'].fillna(\"12:00:00 AM\")\n",
    "videopath_sel['frame_start'] = videopath_sel['first_effective_time'].apply(lambda x: get_frame_num(x))\n",
    "videopath_sel['frame_end'] = videopath_sel['last_effective_time'].apply(lambda x: get_frame_num(x))\n",
    "videopath_sel['frame_end'] = videopath_sel['frame_end'].fillna(videopath_sel['length'])\n",
    "videopath_sel['ref_epsg'] = videopath_sel['ref_epsg'].astype(int)\n",
    "videopath_sel['video_date'] = videopath_sel['video_group_started_at'].apply(lambda x: x.split(\" \")[0])\n",
    "videopath_sel['video_section_started_at'] = pd.to_datetime(videopath_sel['video_date']+ \" \" +videopath_sel['video_section_started_at'])\n",
    "videopath_sel.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load full hex locations (overlapping with the historical ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagingfolder = \"../../_data/05_tracking_result_projected/step1_speed_vector\"\n",
    "foldershp = '/Users/yuan/Dropbox (MIT)/whyte_CV/_data/02_siteplan/site_plan_geometry'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all shapes\n",
    "foldershp = '/Users/yuan/Dropbox (MIT)/whyte_CV/_data/02_siteplan/site_plan_geometry'\n",
    "geofiles = os.listdir(foldershp)\n",
    "geofiles = [x for x in geofiles if x.endswith(\".shp\")]\n",
    "geofiles = [\n",
    "    'chestnutstreet.shp',\n",
    " 'downtowncrossing.shp',\n",
    " 'bryant_park1.shp',\n",
    " 'MET.shp',\n",
    " ]\n",
    "\n",
    "\n",
    "def geth3(zones, res):\n",
    "    zones = zones.to_crs(\"EPSG:4326\")\n",
    "    bound = zones.dissolve()\n",
    "    polydict = bound.convex_hull[0].__geo_interface__\n",
    "    polydict[\"coordinates\"] = (tuple((t[0], t[1]) for t in polydict[\"coordinates\"][0]),)\n",
    "    \n",
    "    hexs = h3.polyfill(polydict, res, geo_json_conformant = True)\n",
    "\n",
    "    polygonise = lambda hex_id: Polygon(\n",
    "                                    h3.h3_to_geo_boundary(\n",
    "                                        hex_id, geo_json=True)\n",
    "                                        )\n",
    "\n",
    "    all_polys = gpd.GeoSeries(list(map(polygonise, hexs)), \\\n",
    "                                          index=hexs, \\\n",
    "                                          crs=\"EPSG:4326\" \\\n",
    "                                         )\n",
    "\n",
    "\n",
    "    h3_all = gpd.GeoDataFrame({\"geometry\": all_polys,\n",
    "                                \"hex_id\": all_polys.index},\n",
    "                                    crs=all_polys.crs\n",
    "                                   )\n",
    "    \n",
    "    return h3_all\n",
    "\n",
    "shp_ls = []\n",
    "for f in geofiles:\n",
    "    shp = gpd.read_file(os.path.join(foldershp, f))\n",
    "    shp = shp.to_crs(epsg=4326)\n",
    "    df_h3 = geth3(shp, 15)\n",
    "    df_h3['video_location'] = f.split(\".\")[0]\n",
    "    df_h3 = gpd.sjoin(df_h3, shp[['geometry']], how=\"inner\", op='intersects')\n",
    "    shp_ls.append(df_h3)\n",
    "    \n",
    "shpdf = pd.concat(shp_ls).reset_index(drop = True)\n",
    "loc_map2 = dict(zip(\n",
    "    shpdf['video_location'].unique(),\n",
    "    ['Chestnut Street', 'Downtown Crossing', 'Bryant Park', 'MET']\n",
    "))\n",
    "shpdf['location_name'] = shpdf['video_location'].apply(lambda x: loc_map2[x])\n",
    "shpdf.to_file(os.path.join(foldershp, \"final_h3_15_four_city.geojson\"), driver = \"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3folder = \"../_data/05_tracking_result_projected/step5_agg_h3\"\n",
    "# stagingfolder = \"../../_data/05_tracking_result_projected/step1_speed_vector\"\n",
    "stagingfolder = \"../../_data/05_tracking_result_projected/step1_speed_vector_05\"\n",
    "foldershp = '/Users/yuan/Dropbox (MIT)/whyte_CV/_data/02_siteplan/site_plan_geometry'\n",
    "shpdf = gpd.read_file(os.path.join(foldershp, \"final_h3_15_four_city.geojson\"))\n",
    "shpdf.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate unique number of people appeared for each frame on a time series\n",
    "# load a sample video first\n",
    "def load_video(videoname):\n",
    "\n",
    "    loc_name = videopath_sel[videopath_sel['video_id']==videoname]['video_location'].values[0]\n",
    "    video_start_frame = videopath_sel[videopath_sel['video_id']==videoname]['frame_start'].values[0]\n",
    "    video_start_at = videopath_sel[videopath_sel['video_id']==videoname]['video_section_started_at'].values[0]\n",
    "    video_group = videopath_sel[videopath_sel['video_id']==videoname]['video_group'].values[0]\n",
    "\n",
    "    # destfolder = os.path.join(outputfolder, loc_name)\n",
    "    # traceGDF = pd.read_csv(os.path.join(destfolder, f\"{videoname}_projected.csv\"))\n",
    "    traceGDF = pd.read_csv(os.path.join(stagingfolder, f\"{videoname}.csv\")) # this data contain speed update already\n",
    "    traceGDF = traceGDF[traceGDF['frame_id']>video_start_frame].reset_index(drop = True)\n",
    "    traceGDF['timestamp'] = video_start_at + traceGDF['frame_id'].apply(lambda x: pd.Timedelta(seconds = x/29.97))\n",
    "    traceGDF['video_group'] = video_group\n",
    "    traceGDF['video_location'] = loc_name\n",
    "    return traceGDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished = ['20100612-120118b01',\n",
    " '20100612-120118b02',\n",
    " '20100612-120118b03',\n",
    " '20100612-120118b04']\n",
    "videols = videopath_sel['video_id'].unique().tolist()\n",
    "videols = [x for x in videols if x not in finished]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Total to process: \", len(videols))\n",
    "fullGDF = []\n",
    "for videoname in tqdm(videols):\n",
    "    try:\n",
    "        traceGDF = load_video(videoname)\n",
    "        traceGDF['video_location'] = videopath_sel[videopath_sel['video_id']==videoname]['video_location'].values[0]\n",
    "        fullGDF.append(traceGDF)\n",
    "    except:\n",
    "        print(videoname, \": failed\")\n",
    "fullGDF = pd.concat(fullGDF).reset_index(drop = True)\n",
    "# fullGDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del traceGDF\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fullGDF['date'] = fullGDF['timestamp'].apply(lambda x: x.date())\n",
    "fullGDF['video_group_update'] = fullGDF['video_location'].apply(lambda x: x.split(\" \")[0])+fullGDF['video_group']\n",
    "# aggregate pedestrian count per location per date per hour by every 5 second\n",
    "fullGDF['second_from_start'] = fullGDF['timestamp'].apply(lambda x: x.hour*3600 + x.minute*60 + x.second)\n",
    "fullGDF['second_start'] = fullGDF.groupby(['video_group_update'])['second_from_start'].transform('min')\n",
    "fullGDF['second_from_start'] = fullGDF['second_from_start'] - fullGDF['second_start']\n",
    "fullGDF = fullGDF[fullGDF['video_group_update']!='Met20100612-124938'].reset_index(drop = True)\n",
    "fullGDF['time_group'] = fullGDF['video_group_update'].apply(lambda x: timedict[x])\n",
    "fullGDF['hex_id'] = fullGDF.apply(lambda x: h3.geo_to_h3(x['lat'], x['lon'], 15), axis = 1)\n",
    "fullGDF['inside'] = fullGDF['hex_id'].apply(lambda x: x in list(shpdf['hex_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selcols = ['track_id', 'frame_id', 'side', 'glasses', 'hat',\n",
    "       'hold_objects_in_front', 'bag', 'upper', 'lower', 'boots', 'loc_x',\n",
    "       'loc_y', 'x_3857', 'y_3857', 'video_id', 'moving_x', 'moving_y', 'lat',\n",
    "       'lon', 'lat_moving', 'lon_moving', 'gender', 'age', \n",
    "       'timestamp', 'video_group', 'videoname', 'move_m_0.5s', 'speed_0.5s',\n",
    "       'dist_x_0.5s', 'dist_y_0.5s', 'speed_x_0.5s', 'speed_y_0.5s', 'video_location',\n",
    "       'date', 'video_group_update',\n",
    "       'second_from_start', 'second_start', 'time_group', 'hex_id', 'inside','track_id_backup']\n",
    "exportcols = [x for x in selcols if x in fullGDF.columns]\n",
    "for video_group in fullGDF['video_group_update'].unique():\n",
    "       temp = fullGDF[fullGDF['video_group_update']==video_group].reset_index(drop = True)\n",
    "       temp[exportcols].to_csv(os.path.join(staginging_folder2, f\"{video_group}_full.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selvideogroups = ['Chestnut20100519-083343',\n",
    " 'Downtown20100521-115755',\n",
    " 'Met20100612-120118',\n",
    " 'bryant_park20081008-141944']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from results above directly\n",
    "fullGDF  = []\n",
    "for video_group in tqdm(selvideogroups):\n",
    "    temp = pd.read_csv(os.path.join(staginging_folder2, f\"{video_group}_full.csv\"))\n",
    "    fullGDF.append(temp)\n",
    "fullGDF = pd.concat(fullGDF).reset_index(drop = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Overall summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {'Bryant Park': '#f77189',\n",
    " 'Chestnut Street': '#97a431',\n",
    " 'Downtown Crossing': '#36ada4',\n",
    " 'MET': '#a48cf4'}\n",
    "namedict = dict(zip(['Chestnut Street videos (NEW)', 'Downtown Crossing videos (NEW)',\n",
    "       'Met Steps videos (NEW)', 'bryant_park'],\n",
    "                    ['Chestnut Street',\n",
    "                     'Downtown Crossing',\n",
    "                     'MET',\n",
    "                     'Bryant Park']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count h3 hexagon by each location\n",
    "shpagg = shpdf.groupby('video_location')['hex_id'].nunique().reset_index()\n",
    "shpagg = shpagg.rename(columns = {'hex_id':'hexagon_count'})\n",
    "shpagg['area'] = shpagg['hexagon_count']*0.889\n",
    "shpagg['video_location'] = np.where(shpagg['video_location']=='bryant_park1', 'bryant_park', shpagg['video_location'])\n",
    "shpagg['video_location'] = shpagg['video_location'].apply(lambda x: x.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 29.97\n",
    "summary_video = fullGDF[fullGDF['inside']==True].groupby(['video_group_update','video_id']).agg({\n",
    "    'track_id':['nunique', 'count'],\n",
    "    'frame_id':'nunique',\n",
    "}).reset_index()\n",
    "summary_video.columns = ['video_group_update', 'video_id', 'pedestrian_count_unique', 'pedestrian_count_total', 'frame_count']\n",
    "summary_video['second_count'] = summary_video['frame_count']/fps\n",
    "summary_video['second_count'] = summary_video['second_count'].astype(int)\n",
    "summary_video['minute_count'] = summary_video['second_count']/60\n",
    "\n",
    "# summary_video['pedestrian_per_min'] = summary_video['pedestrian_count']/summary_video['minute_count']\n",
    "summary_loc = summary_video.groupby('video_group_update').agg({\n",
    "    'pedestrian_count_unique':'sum',\n",
    "    'pedestrian_count_total':\"sum\",\n",
    "    'minute_count':'sum',\n",
    "    \"frame_count\":'sum'\n",
    "}).reset_index()\n",
    "summary_loc['pedestrian_per_min'] = summary_loc['pedestrian_count_unique']/summary_loc['minute_count']\n",
    "summary_loc['time_group'] = summary_loc['video_group_update'].apply(lambda x: timedict[x])\n",
    "summary_loc['video_location'] = summary_loc['video_group_update'].apply(lambda x: x[:-15].title())\n",
    "# summary_loc['area'] = [136.017, 388.493, 443.611, 241.808]\n",
    "# summary_loc = summary_loc.merge(shpagg, on = 'video_location', how = 'left')\n",
    "summary_loc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallsum_f = '/Users/yuan/Dropbox (MIT)/whyte_CV/_data/10_clean/01_timeseries'\n",
    "summary_loc[summary_loc['video_group_update']!='Met20100612-112553'].to_csv(os.path.join(overallsum_f, \n",
    "                                                                                         'pedestrian_overall_2010s.csv'), index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Summary time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedes_temp = fullGDF[fullGDF['inside']==True].groupby(['video_location',\n",
    "                              'video_group_update',\n",
    "                              'video_id','date',\n",
    "                              'second_from_start',\n",
    "                              'timestamp',\n",
    "                              'frame_id']).agg({\n",
    "                                  'track_id': 'nunique'}).reset_index().rename(columns = {'track_id':'pedestrian_count'}).sort_values(\n",
    "                                      ['video_id','timestamp']\n",
    "                                  )\n",
    "pedes_temp['time_group'] = pedes_temp['video_group_update'].apply(lambda x: timedict[x])\n",
    "# get rolling average of track_id\n",
    "\n",
    "# get rolling average of every 30 seconds window\n",
    "window = 60\n",
    "pedes_temp[f'pedestrian_count_rolling_{window}s'] = pedes_temp.groupby(['video_group_update'])['pedestrian_count']\\\n",
    "    .transform(lambda x: x.rolling(window*int(fps),1).mean())\n",
    "\n",
    "pedes_temp['minute_from_start'] = pedes_temp['second_from_start'].apply(lambda x: int(x//60))\n",
    "pedes_temp_min = pedes_temp.groupby(['time_group',\n",
    "                                     'video_location',\n",
    "                                     'video_group_update',\n",
    "                                     'minute_from_start'])[f'pedestrian_count_rolling_{window}s'].mean().reset_index()\n",
    "pedes_temp_min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the data every 1 minute\n",
    "pedes_temp_min['pedestrian_count_rolling_10m'] = \\\n",
    "    pedes_temp_min.groupby(['video_group_update'])['pedestrian_count_rolling_60s'].transform(lambda x: x.rolling(10, 1).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizsel = [\n",
    "\n",
    "       'Downtown20100521-115755', \n",
    "       # 'Met20100612-112553',\n",
    "       'Chestnut20100519-083343',\n",
    "       'Met20100612-120118', \n",
    "       'bryant_park20081008-141944'\n",
    "       ]\n",
    "pedes_temp_min['location_name'] = pedes_temp_min['video_location'].apply(lambda x: namedict[x])\n",
    "vizdata = pedes_temp_min[pedes_temp_min['video_group_update'].isin(vizsel)].reset_index(drop = True)\n",
    "# vizdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.pointplot(data = vizdata, \n",
    "             x=\"minute_from_start\", \n",
    "             y=f\"pedestrian_count_rolling_10m\", \n",
    "             hue='location_name', \n",
    "             ci = 95,\n",
    "             palette=color_dict, \n",
    "             linewidth=2.5, \n",
    "             scale = 0.5,\n",
    "             ax = ax)\n",
    "# add xlabel\n",
    "ax.set_xlabel(\"Minute from start\",fontsize=15)\n",
    "ax.set_ylabel(\"Observed pedestrian\",fontsize=15)\n",
    "# put legend outside the plot\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "ax.set_xticks(np.arange(0, pedes_temp['minute_from_start'].max(), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data for visualization cross time\n",
    "cleanfolder = '../../_data/10_clean/01_timeseries'\n",
    "pedes_temp_min.to_csv(os.path.join(cleanfolder, 'pedestrian_per_min_2010s.csv'), index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. H3 updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3folder = \"../../_data/05_tracking_result_projected/step5_agg_h3_frame\"\n",
    "\n",
    "videols = videopath_sel['video_id'].unique().tolist()\n",
    "# videols = ['20100612-120118b01',\n",
    "#  '20100612-120118b02',\n",
    "#  '20100612-120118b03',\n",
    "#  '20100612-120118b04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# video_todo = [x for x in videols if '20100612-120118' in x]\n",
    "video_todo = videols.copy()\n",
    "for videoname in tqdm(video_todo):\n",
    "    try:\n",
    "        loc_name = videopath_sel[videopath_sel['video_id']==videoname]['video_location'].values[0]\n",
    "        destfolder = os.path.join(outputfolder, loc_name)\n",
    "        traceGDF = pd.read_csv(os.path.join(destfolder, f\"{videoname}_projected.csv\"))\n",
    "\n",
    "        res = 15\n",
    "        traceGDF[f\"h3_{res}\"] = traceGDF.apply(lambda row: h3.geo_to_h3(row[\"lat\"], row[\"lon\"], res), axis = 1)\n",
    "        countpeople = traceGDF.groupby([f\"h3_{res}\",\"frame_id\"])[\"track_id\"].nunique().reset_index()\n",
    "        countpeople = countpeople.rename(columns = {\"track_id\":\"pedestrian_count\"})\n",
    "        countpeople.to_csv(os.path.join(h3folder, f\"{videoname}_h3_frame.csv\"), index = False)\n",
    "    except:\n",
    "        print(videoname, \": failed\")\n",
    "        \n",
    "h3folder = \"../../_data/05_tracking_result_projected/step5_agg_h3_frame\"\n",
    "videols = videopath_sel['video_id'].unique().tolist()\n",
    "countdf = []\n",
    "for videoname in tqdm(videols):\n",
    "    try:\n",
    "        countpeople = pd.read_csv(os.path.join(h3folder, f\"{videoname}_h3_frame.csv\"))\n",
    "        countpeople['videoname'] = videoname\n",
    "        countdf.append(countpeople)\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "countdf = pd.concat(countdf).reset_index(drop = True)\n",
    "countdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map = dict(zip(videopath_sel['video_id'], videopath_sel['video_location']))\n",
    "countdf['video_location'] = countdf['videoname'].apply(lambda x: location_map[x])\n",
    "countdf['video_group_update'] = countdf['video_location'].apply(lambda x: x.split(\" \")[0])+countdf['videoname']\n",
    "countdf.rename(columns = {'track_id':'pedestrian_count'}, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## need to load the shapefile and construct all valid h3 so that we keep the places with 0 observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countdf['video_location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_map3 = dict(zip(\n",
    "    countdf['video_location'].unique(),\n",
    "    ['Bryant Park', 'Chestnut Street',\n",
    "       'Downtown Crossing', 'MET']\n",
    "))\n",
    "countdf['location_name'] = countdf['video_location'].apply(lambda x: loc_map3[x])\n",
    "countdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videols = countdf['videoname'].unique()\n",
    "videols = ['20100612-120118b01',\n",
    " '20100612-120118b02',\n",
    " '20100612-120118b03',\n",
    " '20100612-120118b04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for videoname in videols:\n",
    "    temp = countdf[countdf['videoname']==videoname].reset_index(drop = True).rename(columns = {'h3_15':'hex_id'})\n",
    "    loc = temp['location_name'].unique()[0]\n",
    "    tempshp = shpdf[shpdf['location_name']==loc].reset_index(drop = True)\n",
    "    # construct a dataframe with each frame and hex_id\n",
    "    frame_shp = tempshp[['hex_id']].merge(temp[['frame_id']].drop_duplicates(), how = \"cross\")\n",
    "    print(frame_shp.shape)\n",
    "    mdf = frame_shp[['hex_id','frame_id']].merge(temp,how = \"left\", on = [\"hex_id\", \"frame_id\"]).fillna(0)\n",
    "    mdf.to_csv(os.path.join(h3folder, f\"{videoname}_h3_frame_full.csv\"), index = False)\n",
    "    # fulldf_hx.append(mdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select the videos used in the study\n",
    "countdf['video_group'] = countdf['video_group_update'].apply(lambda x: x[:-3])\n",
    "countdf = countdf[countdf['video_group'].isin(selvideogroups)].reset_index(drop = True)\n",
    "# countdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset and aggregate by each hex_id\n",
    "h3folder = \"../../_data/05_tracking_result_projected/step5_agg_h3_frame\"\n",
    "h3_clean = \"../../_data/10_clean/02_h3_agg\"\n",
    "\n",
    "allloc = ['Bryant Park', 'Chestnut Street',\n",
    "       'Downtown Crossing', 'MET']\n",
    "for location_name in [\"MET\"]:\n",
    "    # videols = countdf[countdf['location_name']==location_name]['videoname'].unique().tolist()\n",
    "    videols = ['20100612-120118b01',\n",
    " '20100612-120118b02',\n",
    " '20100612-120118b03',\n",
    " '20100612-120118b04']\n",
    "    aggdf = []\n",
    "    for videoname in tqdm(videols):\n",
    "        filename = f\"{videoname}_h3_frame_full.csv\"\n",
    "        temp = pd.read_csv(os.path.join(h3folder, filename))\n",
    "        temp_summary = temp.groupby('hex_id').agg({\n",
    "            'pedestrian_count':'sum',\n",
    "            'frame_id':'nunique'\n",
    "        }).reset_index().rename(columns = {'frame_id':'frame_count',\n",
    "                                        'pedestrian_count':'pedestrian_count_sum'})\n",
    "        aggdf.append(temp_summary)\n",
    "    aggdf = pd.concat(aggdf).reset_index(drop = True)\n",
    "    aggdf = aggdf.groupby('hex_id').agg({\n",
    "        'pedestrian_count_sum':'sum',\n",
    "        'frame_count':'sum'\n",
    "    }).reset_index()\n",
    "    aggdf['pedestrian_count_frame'] = aggdf['pedestrian_count_sum']/aggdf['frame_count']\n",
    "    aggdf['location_name'] = location_name\n",
    "    # aggdf.to_csv(os.path.join(h3_clean, f\"{location_name}_h3_agg.csv\"), index = False)\n",
    "    # merge the geometry and check result\n",
    "    viz = shpdf.merge(aggdf, on = \"hex_id\", how = \"inner\")\n",
    "    viz.plot(column = \"pedestrian_count_frame\", legend = True)\n",
    "    viz.to_file(os.path.join(h3folder, f\"{location_name}_2010s_h3_agg.geojson\"), driver = \"GeoJSON\")\n",
    "    viz.drop(\"geometry\", axis = 1).to_csv(os.path.join(h3folder, f\"{location_name}_2010s_h3_agg.csv\"), index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = []\n",
    "for location_name in ['Bryant Park', 'Chestnut Street',\n",
    "       'Downtown Crossing', 'MET']:\n",
    "    viz = gpd.read_file(os.path.join(h3folder, f\"{location_name}_2010s_h3_agg.geojson\"))\n",
    "    overall.append(viz)\n",
    "overall = pd.concat(overall).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation to get unique track_id per second\n",
    "fullGDF_agg_sec = fullGDF.groupby([\n",
    "    # 'video_id',\n",
    "                                      'video_location',\n",
    "                                      'hex_id',\n",
    "]).agg({\n",
    "                                          \"track_id\":\"nunique\",\n",
    "                                        #   \"second_from_start\":\"nunique\"\n",
    "                                      }).reset_index().rename(columns = {'track_id':'pedestrian_unique_count'})\n",
    "fullGDF_time = fullGDF.groupby('video_location').agg({'second_from_start':'max'})\\\n",
    "  .reset_index().rename(columns = {'second_from_start':'total_second'})\n",
    "fullGDF_agg_sec = fullGDF_agg_sec.merge(fullGDF_time, on = \"video_location\", how = \"left\")\n",
    "                 \n",
    "gdfsummary = overall.merge(fullGDF_agg_sec[['hex_id', 'pedestrian_unique_count', 'total_second']], \n",
    "              on = [\"hex_id\"], how = \"left\").drop('location_name_y', axis = 1).rename(columns = {'location_name_x':'location_name'})\n",
    "gdfsummary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(\n",
    "    data =gdfsummary,\n",
    "    x = \"pedestrian_unique_count\",\n",
    "    y = \"pedestrian_count_frame\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfsummary.to_file(os.path.join(h3_clean, f\"2010s_h3_agg.geojson\"), driver = \"GeoJSON\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot timeseries change of people observed (Archived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videopath_sel['video_section'] = videopath_sel['video_id'].apply(lambda x: x[-2:])\n",
    "videopath_sel = videopath_sel[videopath_sel['video_section']!=\"T)\"].reset_index(drop= True)\n",
    "videopath_sel['video_section'] = videopath_sel['video_section'].astype(int)\n",
    "videopath_sel['video_section'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videopath_sel = videopath_sel.sort_values([\"video_location\", \"video_section\"], ascending = True)\n",
    "videopath_sel['starting_time'] = videopath_sel['video_name'].apply(lambda x: x.split(\"b\")[0])\n",
    "videopath_sel['starting_time'] = pd.to_datetime(videopath_sel['starting_time'])\n",
    "# the starting_time is only valid for the first video in each location\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Gender Distribution among four videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual analysis, use overall data per place to generate\n",
    "import seaborn as sns\n",
    "videols = [ \"20081008-141944b02\", \"20100519-083343b03\", \"20100521-074701b03\",\"20100612-120118b01\"]\n",
    "for videoname in videols:\n",
    "    loc_name = videopath_sel[videopath_sel['video_id']==videoname]['video_location'].values[0]\n",
    "    print(f\"{loc_name = }\")\n",
    "    destfolder = os.path.join(outputfolder, loc_name)\n",
    "    traceGDF = pd.read_csv(os.path.join(destfolder, f\"{videoname}_projected.csv\"))\n",
    "    gendersummary = traceGDF.groupby(\"gender\")[\"track_id\"].nunique().reset_index()\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (5,5))\n",
    "    sns.barplot(\n",
    "        data = gendersummary,\n",
    "        x = \"gender\", \n",
    "        y = \"track_id\",\n",
    "        palette = [\"#ef5c43\", \"#3bc0cf\"]\n",
    "        \n",
    "    )\n",
    "    sns.despine()\n",
    "    ax.set_xlabel(\"Gender\")\n",
    "    ax.set_ylabel(\"Number of pedestrians\")\n",
    "    ax.grid(axis = \"y\", color = \"grey\", linestyle = \"--\", linewidth = 0.5)\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_tick_params(rotation=0)\n",
    "    plt.xticks(fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fdb9f36a2fa6c0d80c32614b079baf171674cda7a504cb8efc605d7162d1d77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
